{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ced188",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Nathan Willemsen\n",
    "\n",
    "DS552 - Generative AI\n",
    "\n",
    "Prof. Narahara Chari Dingari, Ph.D.\n",
    "\n",
    "January 16, 2025\n",
    "\n",
    "\n",
    "1. Theory Questions:\n",
    "- Q1: Why is the KL Divergence term important in the VAE loss function?\n",
    "- Q2: How does the reparameterization trick enable backpropagation through the stochastic layers of a VAE?\n",
    "- Q3: Why does a VAE use a probabilistic latent space instead of a fixed latent space?\n",
    "- Q4: What role does KL Divergence play in ensuring a smooth latent space?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2cc218",
   "metadata": {},
   "source": [
    "2. Coding Tasks:\n",
    "- Task 1: Modify the VAE architecture to use convolutional layers for both the encoder and decoder, and train it on the CIFAR-10 dataset. This modification will allow the model to capture spatial relationships within images more effectively, improving its ability to generate high-quality images. After training, compare the generated images with those from a fully connected VAE.\n",
    "- Task 2: Using the trained VAE, interpolate between two images in the latent space and generate intermediate images. This demonstrates how smoothly the model can transition between different data points. Visualize and display the results, showing the interpolated images in a grid format to observe the transformation.\n",
    "- Task 3: Train the VAE on a new dataset of your choice (e.g., CelebA for faces), and visualize generated samples. Experiment with sampling from different regions of the latent space and analyze how the generated outputs vary based on different latent vectors.\n",
    "- Coding Task Explanation: In this assignment, the VAE will be expanded by incorporating convolutional layers in both the encoder and decoder networks. Convolutional layers are particularly effective for image\n",
    "data, as they can capture spatial hierarchies better than fully connected layers. By using this architecture on the CIFAR-10 dataset (a dataset of small, colorful images), the VAE will be able to learn a more effective latent representation, which should improve the quality of generated images. The second task involves interpolation in the latent space. By interpolating between two points in the latent space (corresponding to two different images), we can observe how smoothly the VAE transitions between two\n",
    "images. This task highlights the structure of the learned latent space and shows the generative capabilities of the VAE.\n",
    "Finally, applying the VAE to a new dataset (such as CelebA, a dataset of celebrity faces) and visualizing the generated images allows us to see how well the model generalizes to different kinds of data and how the latent space is structured for more complex, real-world images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbf54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5942424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723c8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x): #TODO: Use convolutional layers for encoding and decoding layers\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h2 = torch.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 3072))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103f66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 3072), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3dc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3faa51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1, Loss: 1928.7509884375\n",
      "Epoch 2, Loss: 1865.9978953125\n",
      "Epoch 3, Loss: 1857.5221271875\n",
      "Epoch 4, Loss: 1853.7255390625\n",
      "Epoch 5, Loss: 1850.0748759375\n",
      "Epoch 6, Loss: 1848.29160625\n",
      "Epoch 7, Loss: 1846.091485625\n",
      "Epoch 8, Loss: 1844.2730259375\n",
      "Epoch 9, Loss: 1843.3770471875\n",
      "Epoch 10, Loss: 1842.7212959375\n"
     ]
    }
   ],
   "source": [
    "# TODO: train it on the CIFAR-10 dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Load MNIST dataset and run the training\n",
    "# transform = transforms.ToTensor()\n",
    "# train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "vae = VAE(input_dim=3072, hidden_dim=400, latent_dim=20)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "    \n",
    "for epoch in range(1, 11):\n",
    "    train(vae, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707c8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to generate and display new images\n",
    "def generate_images(model, num_images=10, latent_dim=20):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Turn off gradients for generation\n",
    "        # Sample random points from the latent space (standard normal distribution)\n",
    "        z = torch.randn(num_images, latent_dim)\n",
    "        # Decode these points to generate images\n",
    "        generated_images = model.decode(z).cpu()\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(num_images, 1.5))\n",
    "    for i in range(num_images):\n",
    "        axs[i].imshow(generated_images[i].view(100, 100), cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ba3d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 100]' is invalid for input of size 3072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have already trained the model (vae) and it has a latent dimension of 20\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generate_images(vae, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mgenerate_images\u001b[0;34m(model, num_images, latent_dim)\u001b[0m\n\u001b[1;32m     13\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, num_images, figsize\u001b[38;5;241m=\u001b[39m(num_images, \u001b[38;5;241m1.5\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images):\n\u001b[0;32m---> 15\u001b[0m     axs[i]\u001b[38;5;241m.\u001b[39mimshow(generated_images[i]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m     axs[i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 100]' is invalid for input of size 3072"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x150 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have already trained the model (vae) and it has a latent dimension of 20\n",
    "generate_images(vae, num_images=5, latent_dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841b025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea04560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
